#!/usr/bin/python

import argparse
import pandas as pd
import os
import pickle

import gff
from ansi import error

# this script is used to generate the two critical files: the sequence annotation
# table (.sat file) and the hierarchy tree file (.hierarchy) in the same 
# directory as the .gff file. this conversion does not lose any information in
# the original gff file. and will be utilized in later calculations by other scripts.

# gff's already generated a .sat and a .hierarchy files under the same directory
# will be automatically ignored by the script.

parser = argparse.ArgumentParser(
    prog = 'annot: gffindex',
    description = 'index gff files to sequence annotation table and hierarchy (.sat, .hierarchy)'
)

parser.add_argument('fname', type = str,
                    help = 'input file, gff file (can be gzipped)')
parser.add_argument('-z', default = False, action = 'store_true', dest = 'gzipped',
                    help = 'explicitly specify whether the file is zipped archive')

params = parser.parse_args()

search_sat_name = params.fname.rstrip('.gz') + '.sat'
search_sac_name = params.fname.rstrip('.gz') + '.sac'
search_hierarchy_name = params.fname.rstrip('.gz') + '.hierarchy'

if params.fname.endswith('gz'):
    params.gzipped = True

# dump sat files: there is no data loss.
if not os.path.exists(search_sat_name):

    contents = None
    if params.gzipped: contents = gff.readgff_gzipped(params.fname)
    else: contents = gff.readgff(params.fname)

    if contents == None:
        error('read gff file error.')

    with open(search_sat_name, 'wb') as f:
        pickle.dump(contents, f)

else:
    
    with open(search_sat_name, 'rb') as f:
        contents = pickle.load(f)

# construct hierarchy table.
if not os.path.exists(search_hierarchy_name):
    
    # a plain lookup table containing ids as values (direct key). we have checked
    # that an ID field in the metadata is never absent. at least for human.

    unmapped = [x for x in range(len(contents['.seqid']))]
    root = { 'id': -1 }
    lookup_hierach = { '.root': root }

    niter = 0
    while len(unmapped) > 0:

        niter += 1
        print('constructing tree: iteration {0}'.format(niter))

        thisiter = [x for x in unmapped]
        xiter = 0
        removed = []
        for x in thisiter:

            xiter += 1

            if xiter % 1000 == 0:
                print('\riterating {0} out of {1}'.format(xiter, len(thisiter)), end = '', flush = True)

            if contents['Parent'][x] == None:

                if not contents['.type'][x] in root.keys(): root[contents['.type'][x]] = []
                obj = { 'id': x }
                root[contents['.type'][x]] += [obj]
                lookup_hierach[contents['ID'][x]] = obj
                removed += [x]

            else:

                parent = contents['Parent'][x]
                if not parent in lookup_hierach.keys(): continue
                pobj = lookup_hierach[parent]

                if not contents['.type'][x] in pobj.keys(): pobj[contents['.type'][x]] = []
                obj = { 'id': x }
                pobj[contents['.type'][x]] += [obj]
                lookup_hierach[contents['ID'][x]] = obj
                removed += [x]

            pass

        print('')
        unmapped = list(set(unmapped) - set(removed))

    del lookup_hierach
    with open(search_hierarchy_name, 'wb') as f:
        pickle.dump(root, f)

else: pass

if not os.path.exists(search_sac_name):

    if params.gzipped: coltotal, coltyped = gff.gffcols_gzipped(params.fname)
    else: coltotal, coltyped = gff.gffcols(params.fname)
    with open(search_sac_name, 'wb') as f:
        pickle.dump(coltyped, f)

else: pass
